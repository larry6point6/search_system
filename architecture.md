# Semantic Search

I've been asked to architect the design of a semantic search solution, there are a number of components which will be required in order to create this. This document is WIP and will change as the project is developed. 

1. An Ingestion Component
2. Some kind of Transformation(Embedding)
3. Storage
4. Search Tuning
5. Search is customizable
6. Date Tuning
7. Some ranking/reccomendation system/approach

## Getting the information we want to be searchable

Working on the premise of needing to gather data from a number of websites, if the urls are available upfront then a script that will scrape the data from these websites can be written. I'm leaning towards using Python here for this as it has a strong ecosystem around web scraping. Think the web scraping approach allows for the most flexibility.

If we need to find the data(not known upfront), then a combination of web scraping and web crawling is most likely required. The crawling to generate a list of URLS of interest, then the scraping to extract the relevant data. If something more generic is required that could be used to create a semantic search solution for any corpus, then I think the combination of web crawling and scraping is a good way to achieve this.

**TODO** - Write module(s) that achieve the above

## Embedding the information

The information scraped will need to be embedded, this will be done using ```SentenceTransformers```. Once the embeddings have been created, that allows for the initial testing/exploration of the semantic search portion. This is all documented [here](./movies_semantic_search.ipynb)

The initial testing will be done using a small corpus of movie titles and their plot details(circa 10k), as the project develops this will be expanded out for bigger use cases, currently this is housed in. 

Currently the information is embedded in an index file, generated by the ```FAISS``` library. 

Some questions that will need further exploration:

Will the corpus need to be periodically updated?
Does there need to some record of updates to the corpus?
What if a documment is modified how is that going to be tracked?
How often should this check take place? 

## Model experiments

Currently the first pass is very rough, it does return some results but they aren't very good, the model will need some finetuning.

**TODO** - Finetune Model, improve responses, what steps are needed to make the results better. 


## Storage

Currently a sample dataframe of 10k records is converted and written out to an index file, then is later read back in for use in the semantic search portion of this project. 

In rudimentary testing first crack, the inital embeddding seems to be taking some time, at the moment dropping in a file for ease of exploration, does moving to a database solve this?

**TODO** - Explore, how a database could improve this and possibly update the embeddings in batches. 

Also is the test dataset too large, (circa 10k seems to be fine)?

